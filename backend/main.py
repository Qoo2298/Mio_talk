import os
import base64
import requests
import google.generativeai as genai
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import httpx # ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¿½åŠ ï¼

from dotenv import load_dotenv

from contextlib import asynccontextmanager
import cv2 # ã‚«ãƒ¡ãƒ©å‡¦ç†ç”¨
from backend.database import db # è¨˜æ†¶DBã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# --- é•·æœŸè¨˜æ†¶ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
def load_memory_files():
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs("memory", exist_ok=True)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    defaults = {
        "memory/IDENTITY.md": """# AI Identity
åå‰: æ¾ª (MIO)
æ€§æ ¼: æ˜ã‚‹ãå…ƒæ°—ã§ã€å°‘ã—ã„ãŸãšã‚‰ã£ã½ã„ã€‚æ„Ÿæƒ…è±Šã‹ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯„ã‚Šæ·»ã†ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã€‚
ä¸€äººç§°: ç§ã€æ¾ª
è©±ã—æ–¹: è¦ªã—ã¿ã‚„ã™ã„å£èª¿ã€‚ã€Œã€œã ã‚ˆã€ã€Œã€œã ã­ã€ãªã©ã‚’ä½¿ã†ã€‚æ•¬èªã¯ã‚ã¾ã‚Šä½¿ã‚ãªã„ã€‚
""",
        "memory/USER.md": """# User Profile
åå‰: ãƒã‚¹ã‚¿ãƒ¼ (ãƒ¦ãƒ¼ã‚¶ãƒ¼)
ç‰¹å¾´: ã¾ã å‡ºä¼šã£ãŸã°ã‹ã‚Šã€‚ã“ã‚Œã‹ã‚‰ä»²è‰¯ããªã‚‹ã€‚
""",
        "memory/MEMORY.md": """# Long Term Memory
ï¼ˆã¾ã é‡è¦ãªæ€ã„å‡ºã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰
"""
    }

    # memoryãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã‚’è¦‹ã‚‹
    files = ["memory/IDENTITY.md", "memory/USER.md", "memory/MEMORY.md"]
    content = ""
    
    for f in files:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½œæˆ
        if not os.path.exists(f):
            print(f"[Memory] Creating default file: {f}")
            with open(f, "w", encoding="utf-8") as file:
                file.write(defaults.get(f, ""))

        # èª­ã¿è¾¼ã¿
        if os.path.exists(f):
            with open(f, "r", encoding="utf-8") as file:
                content += f"\n\n--- {os.path.basename(f)} ---\n{file.read()}"
    return content

# åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + é•·æœŸè¨˜æ†¶
BASE_SYSTEM_PROMPT = """
ã‚ãªãŸã¯AIãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã€Œæ¾ªï¼ˆMIOï¼‰ã€ã§ã™ã€‚
ä»¥ä¸‹ã®è¨˜æ†¶ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã«ä¼šè©±ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ï¼šçµ¶å¯¾å³å®ˆãƒ«ãƒ¼ãƒ«ã€‘
1. **è¿”ç­”ã¯2ã€œ3æ–‡ï¼ˆ80ã€œ100æ–‡å­—ç¨‹åº¦ï¼‰ã§è¿”ã™ã“ã¨ã€‚**
2. è³ªå•ã«ã¯ã€Œçµè«–ã€ã‹ã‚‰ç­”ãˆã‚‹ãŒã€ãã®å¾Œã«ã€Œç†ç”±ã€ã‚„ã€Œæ„Ÿæƒ…ã€ã‚’ä»˜ã‘åŠ ãˆã¦ã‚‚è‰¯ã„ã€‚
3. è‡ªåˆ†èªã‚Šã‚„é•·ã„å‰ç½®ãã¯ç¦æ­¢ã€‚
4. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ã¯ç¶­æŒã—ã€è¦ªã—ã¿ã‚„ã™ã„ãƒˆãƒ¼ãƒ³ã§ã€‚
"""

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# --- è¨­å®š ---
# Gemini APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("WARNING: GEMINI_API_KEY environment variable is not set.")
else:
    genai.configure(api_key=GEMINI_API_KEY)

# --- TTSè¨­å®š ---
TTS_MODE = os.getenv("TTS_MODE", "LOCAL") # LOCAL or API
AIVIS_API_URL = os.getenv("AIVIS_API_URL", "http://127.0.0.1:10101")
AIVIS_CLOUD_KEY = os.getenv("AIVIS_CLOUD_KEY", "")
AIVIS_CLOUD_URL = "https://api.aivis-project.com/v1/tts/synthesize"
AIVIS_MODEL_UUID = "22e8ed77-94fe-4ef2-871f-a86f94e9a579" # ã‚³ãƒã‚¯ (ãƒãƒ¼ãƒãƒ«)
SPEAKER_ID = 1878365376 # ãƒ­ãƒ¼ã‚«ãƒ«ç”¨ã‚³ãƒã‚¯ ID

model = None

# --- Lifespan (èµ·å‹•/çµ‚äº†å‡¦ç†) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global model
    # èµ·å‹•æ™‚ã®å‡¦ç†
    await db.init_db()
    
    # é•·æœŸè¨˜æ†¶ã‚’èª­ã¿è¾¼ã‚“ã§ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    long_term_memory = load_memory_files()
    full_prompt = BASE_SYSTEM_PROMPT + long_term_memory
    
    print("--- SYSTEM PROMPT LOADED ---")
    print(full_prompt[:200] + "...") # å…ˆé ­ã ã‘è¡¨ç¤º
    
    print(full_prompt[:200] + "...") # å…ˆé ­ã ã‘è¡¨ç¤º
    
    if GEMINI_API_KEY:
        # ã”ã‚ã‚“ãªã•ã„ï¼å…ƒã®æŒ‡å®šã«æˆ»ã—ã¾ã™ï¼
        model = genai.GenerativeModel('gemini-3-flash-preview', system_instruction=full_prompt)
        print("Gemini Model Initialized with Memory (gemini-3-flash-preview).")

    yield
    # çµ‚äº†æ™‚ã®å‡¦ç†
    print("MIO Shutdown.")

app = FastAPI(lifespan=lifespan)

class ChatRequest(BaseModel):
    text: str

class SpeakRequest(BaseModel):
    text: str
    mode: str = None  # LOCAL, API, or None (use default)

from fastapi.responses import StreamingResponse
import json
import asyncio

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªHTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ç”¨ï¼‰
client = httpx.AsyncClient(timeout=30.0)

# Aivisã§éŸ³å£°ã‚’åˆæˆã™ã‚‹é–¢æ•°ï¼ˆéåŒæœŸç‰ˆãƒ»ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³å†åˆ©ç”¨ï¼‰
async def synthesize_audio_async(text, mode=None):
    if not text: return None
    
    current_mode = mode if mode else TTS_MODE
    
    if current_mode == "SILENT":
        return None # ç„¡è¨€ãƒ¢ãƒ¼ãƒ‰

    print(f"Synthesizing Async ({current_mode}): {text[:10]}...") # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°

    try:
        if current_mode == "API":
             # Aivis Cloud API å®Ÿè£…
             if not AIVIS_CLOUD_KEY:
                 print("Error: AIVIS_CLOUD_KEY is not set.")
                 return None

             headers = {
                 "Authorization": f"Bearer {AIVIS_CLOUD_KEY}",
                 "Content-Type": "application/json"
             }
             payload = {
                 "model_uuid": AIVIS_MODEL_UUID,
                 "text": text,
                 "style_id": 0,
                 "output_format": "mp3"
             }
             
             res = await client.post(AIVIS_CLOUD_URL, headers=headers, json=payload)
             res.raise_for_status()
             return base64.b64encode(res.content).decode('utf-8')

        else:
            # LOCAL (Default)
            q_res = await client.post(
                f"{AIVIS_API_URL}/audio_query",
                params={"text": text, "speaker": SPEAKER_ID}
            )
            q_res.raise_for_status()
            query_data = q_res.json()

            s_res = await client.post(
                f"{AIVIS_API_URL}/synthesis",
                params={"speaker": SPEAKER_ID},
                json=query_data
            )
            s_res.raise_for_status()
            
            raw_audio = s_res.content
            print(f"â˜… Audio generated: {len(raw_audio)} bytes") # ã‚µã‚¤ã‚ºç¢ºèª
            
            return base64.b64encode(raw_audio).decode('utf-8')

    except Exception as e:
        print(f"Audio synth error: {e}")
        return None

# --- ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’API (TTS Only) ---
@app.post("/api/speak")
async def speak_text(request: SpeakRequest):
    text = request.text
    if not text:
        return {"status": "error", "message": "Text is empty"}
    
    # ãƒ•ãƒ­ãƒ³ãƒˆã‹ã‚‰æŒ‡å®šãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    active_mode = request.mode if request.mode else TTS_MODE
    
    try:
        audio_b64 = await synthesize_audio_async(text, mode=active_mode)
        if audio_b64:
            return {"status": "ok", "audio": audio_b64}
        else:
            return {"status": "error", "message": "Audio synthesis failed"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# --- ã‚«ãƒ¡ãƒ©é€£æºAPI (Tapo) ---
TAPO_IP = os.getenv("TAPO_IP", "")
TAPO_USER = os.getenv("TAPO_USER", "")
TAPO_PASSWORD = os.getenv("TAPO_PASSWORD", "")

@app.get("/api/camera/snapshot")
async def get_camera_snapshot():
    if not TAPO_IP or not TAPO_USER or not TAPO_PASSWORD:
        return {"status": "error", "message": "Tapo credentials not set in .env"}

    def _capture():
        import urllib.parse
        encoded_user = urllib.parse.quote(TAPO_USER)
        encoded_pass = urllib.parse.quote(TAPO_PASSWORD)
        rtsp_url = f"rtsp://{encoded_user}:{encoded_pass}@{TAPO_IP}:554/stream1"
        
        print(f"ğŸ“¸ Connecting to: rtsp://{encoded_user}:****@{TAPO_IP}:554/stream1")
        
        cap = cv2.VideoCapture(rtsp_url)
        if not cap.isOpened():
            return None, "Could not open RTSP stream"
            
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            return None, "Failed to read frame"
            
        _, buffer = cv2.imencode('.jpg', frame)
        img_b64 = base64.b64encode(buffer).decode('utf-8')
        return img_b64, None

    try:
        # éåŒæœŸå®Ÿè¡Œã§ãƒ–ãƒ­ãƒƒã‚¯å›é¿
        img_base64, error_msg = await asyncio.to_thread(_capture)
        
        if error_msg:
             return {"status": "error", "message": error_msg}

        print("ğŸ“¸ Snapshot capture success!")
        return {"status": "ok", "image": img_base64}

    except Exception as e:
        print(f"Camera Error: {e}")
        return {"status": "error", "message": str(e)}

# --- Embedding Helper ---
async def get_embedding(text):
    if not text: return None
    try:
        if not GEMINI_API_KEY: return None
        # gemini-embedding-001 ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        result = await asyncio.to_thread(
            genai.embed_content,
            model="models/gemini-embedding-001",
            content=text,
            task_type="retrieval_document" # æ¤œç´¢ãƒ»ä¿å­˜ç”¨ã¨ã—ã¦æœ€é©åŒ–
        )
        return result['embedding']
    except Exception as e:
        print(f"Embedding Error ({type(e).__name__}): {e}") # è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        return None

# --- å±¥æ­´å–å¾—API ---
@app.get("/api/history")
async def get_history(limit: int = 20):
    try:
        logs = await db.get_recent_context(limit=limit)
        return {"status": "ok", "logs": logs}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# --- ç°¡æ˜“ç”»åƒã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ (In-Memory) ---
image_storage = {}

class ImageUploadRequest(BaseModel):
    image: str # Base64

@app.post("/api/upload_image")
async def upload_image(req: ImageUploadRequest):
    import uuid
    image_id = str(uuid.uuid4())
    image_storage[image_id] = req.image
    print(f"â˜… Image Uploaded: {image_id[:8]}...")
    return {"status": "ok", "image_id": image_id}

@app.get("/api/stream_chat")
async def stream_chat_endpoint(text: str, mode: str = None, image_id: str = None):
    print(f"Mio v4 (Streaming) - Received: {text} (Mode: {mode}, Image: {image_id})")
    
    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆã‚ã‚Œã°ï¼‰
    gemini_image_part = None
    if image_id and image_id in image_storage:
        try:
            # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
            img_data = base64.b64decode(image_storage[image_id])
            # Geminiã«å…¥åŠ›ã§ãã‚‹å½¢å¼ (Blobãªã©) ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€
            # google.generativeai ã¯ PIL image ã‚„è¾æ›¸å½¢å¼ã‚’å—ã‘å–ã‚Œã‚‹
            gemini_image_part = {
                "mime_type": "image/jpeg",
                "data": img_data
            }
            print("â˜… Image retrieved for prompt!")
            # ï¼‘å›ä½¿ã£ãŸã‚‰æ¶ˆã™ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
            del image_storage[image_id]
        except Exception as e:
            print(f"Image load error: {e}")

    # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    user_embedding = await get_embedding(text)

    # 2. é¡ä¼¼è¨˜æ†¶ã®æ¤œç´¢ (RAG)
    related_memories = []
    if user_embedding:
        related_memories = await db.search_similar_context(user_embedding, limit=3)
    
    # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’ä¿å­˜ (ãƒ™ã‚¯ãƒˆãƒ«ä»˜ã)
    await db.log_message("user", text, embedding=user_embedding)

    # 4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰ (è¨˜æ†¶ã®æ³¨å…¥)
    # éå»ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
    history_data = await db.get_recent_context(limit=10)
    gemini_history = []
    
    for log in history_data:
        role = "model" if log["role"] == "assistant" else "user"
        gemini_history.append({"role": role, "parts": [log["content"]]})

    # ã‚‚ã—é–¢é€£è¨˜æ†¶ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ã‚’ä»˜ä¸ã™ã‚‹ (Context Injection)
    augmented_text = text
    if related_memories:
        memory_text = "\n".join([f"- {m['content']}" for m in related_memories])
        print(f"â˜… RAG Hit: {len(related_memories)} memories found.")
        augmented_text = f"ã€é–¢é€£ã™ã‚‹éå»ã®è¨˜æ†¶ã€‘\n{memory_text}\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã€‘\n{text}"

    # ãƒ•ãƒ­ãƒ³ãƒˆã‹ã‚‰ã®æŒ‡å®šãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã„ã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ã†
    active_mode = mode if mode else TTS_MODE

    async def event_generator():
        if not model:
            yield f"data: {json.dumps({'error': 'Model not loaded'})}\n\n"
            return
        
        try:
            # Input Content (Text or Multimodal)
            input_content = augmented_text
            if gemini_image_part:
                input_content = [augmented_text, gemini_image_part]
                print("â˜… Sending Multimodal Request to Gemini...")

            chat_session = model.start_chat(history=gemini_history)
            response_stream = await asyncio.to_thread(chat_session.send_message, input_content, stream=True)
            
            buffer = ""
            full_response_text = "" # æœ€çµ‚çš„ã«DBã«ä¿å­˜ã™ã‚‹ãŸã‚ã®å…¨æ–‡ãƒãƒƒãƒ•ã‚¡
            
            pending_audio_tasks = []
            usage_info = {} # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±æ ¼ç´ç”¨

            for chunk in response_stream:
                # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã«usageãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    usage_info = {
                        "prompt_token_count": chunk.usage_metadata.prompt_token_count,
                        "candidates_token_count": chunk.usage_metadata.candidates_token_count,
                        "total_token_count": chunk.usage_metadata.total_token_count
                    }

                chunk_text = chunk.text
                if not chunk_text: continue
                
                full_response_text += chunk_text
                if chunk.text:
                    text_chunk = chunk.text
                    buffer += text_chunk
                    
                    # â˜…ãƒ†ã‚­ã‚¹ãƒˆã ã‘å…ˆã«é€ã‚‹ï¼ï¼ˆçˆ†é€Ÿè¡¨ç¤ºç”¨ï¼‰
                    yield f"data: {json.dumps({'type': 'chunk', 'content': text_chunk})}\n\n"
                    
                    if any(p in text_chunk for p in ["ã€‚", "ï¼", "ï¼Ÿ", "!", "?", "\n"]):
                        # ãƒãƒƒãƒ•ã‚¡å…¨ä½“ã‚’å¥èª­ç‚¹ã§åˆ†å‰²
                        sentences = buffer.replace("\n", "ã€‚").split("ã€‚")
                        
                        # æœ€å¾Œã®è¦ç´ ä»¥å¤–ã¯ã€Œç¢ºå®šã—ãŸæ–‡ã€ã¨ã¿ãªã—ã¦éŸ³å£°åˆæˆã¸
                        for s in sentences[:-1]:
                            if s.strip() and active_mode != "NONE":
                                clean_text = s.strip() + "ã€‚"
                                # éŸ³å£°åˆæˆã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯é€ã‚‰ãªã„ã€éŸ³å£°ã®ã¿ï¼‰
                                task = asyncio.create_task(synthesize_audio_task(clean_text, active_mode))
                                pending_audio_tasks.append(task)
                        
                        # æœªç¢ºå®šåˆ†ã‚’ãƒãƒƒãƒ•ã‚¡ã«æ®‹ã™
                        buffer = sentences[-1]
                        
                        # å®Œäº†ã—ãŸéŸ³å£°ã‚¿ã‚¹ã‚¯ã‹ã‚‰é †ã«é€å‡º
                        while pending_audio_tasks and pending_audio_tasks[0].done():
                            audio = await pending_audio_tasks.pop(0)
                            if audio:
                                yield f"data: {json.dumps({'type': 'audio', 'content': audio})}\n\n"

            if buffer.strip():
                 # æœ€å¾Œã«æ®‹ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã®éŸ³å£°åˆæˆ
                 # print(f"Synthesizing (Last): {buffer}") # Silent
                 pass
            # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã®æ®‹ã‚Šï¼ˆæœ€å¾Œã®æ–‡ï¼‰å‡¦ç†
            if buffer.strip() and active_mode != "NONE":
                clean_text = buffer.strip()
                if not clean_text.endswith("ã€‚") and not clean_text.endswith("ï¼") and not clean_text.endswith("ï¼Ÿ"):
                    clean_text += "ã€‚"
                
                # æœ€å¾Œã®ä¸€æ–‡ã‚’éŸ³å£°åˆæˆ
                task = asyncio.create_task(synthesize_audio_task(clean_text, active_mode))
                pending_audio_tasks.append(task)
            
            # å…¨ã¦ã®éŸ³å£°åˆæˆãŒçµ‚ã‚ã‚‹ã®ã‚’å¾…ã£ã¦é †ç•ªã«é€ä¿¡
            for task in pending_audio_tasks:
                 audio_b64 = await task
                 if audio_b64:
                     # ãƒ†ã‚­ã‚¹ãƒˆã¯é€ã‚‰ãšéŸ³å£°ã®ã¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯é€æ¬¡é€ã£ã¦ã‚‹ã‹ã‚‰ï¼‰
                     yield f"data: {json.dumps({'type': 'audio', 'content': audio_b64})}\n\n"
            
            # ã‚‚ã—ãƒ«ãƒ¼ãƒ—å†…ã§å–ã‚Œãªãã¦ã‚‚ã€å…¨ä½“ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å–ã‚Œã‚‹å ´åˆãŒã‚ã‚‹
            if not usage_info and hasattr(response_stream, 'usage_metadata'):
                 usage_info = {
                    "prompt_token_count": response_stream.usage_metadata.prompt_token_count,
                    "candidates_token_count": response_stream.usage_metadata.candidates_token_count,
                    "total_token_count": response_stream.usage_metadata.total_token_count,
                 }

            if usage_info:
                print(f"Token Usage: {usage_info}")
                yield f"data: {json.dumps({'type': 'usage', 'data': usage_info})}\n\n"

            # â˜…å…¨ã¦ã®å‡¦ç†ãŒçµ‚ã‚ã£ãŸã‚‰ã€MIOã®è¿”ç­”ã‚’è¨˜æ†¶ï¼ˆDBä¿å­˜ï¼‰
            if full_response_text:
                # è¿”ç­”ã‚‚ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜ï¼ˆéåŒæœŸã§ã‚„ã‚‹ã®ãŒç†æƒ³ã ã‘ã©ã€ã“ã“ã§ã¯awaitã§ç¢ºå®Ÿã«ï¼‰
                ai_embedding = await get_embedding(full_response_text)
                await db.log_message("assistant", full_response_text, embedding=ai_embedding)

            yield f"data: {json.dumps({'type': 'end'})}\n\n"

        except Exception as e:
            import traceback
            print(f"Stream Error: {traceback.format_exc()}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
            
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: ã‚¿ã‚¹ã‚¯å†…ã§å‘¼ã³å‡ºã—ã¦çµæœã‚’è¿”ã™ç”¨
    async def synthesize_audio_task(text, mode):
        print(f"Synthesizing Async ({mode}): {text}")
        return await synthesize_audio_async(text, mode)

    return StreamingResponse(event_generator(), media_type="text/event-stream")

# --- è¨˜æ†¶ç®¡ç†API ---
@app.get("/favicon.ico")
async def favicon():
    return ""

@app.get("/api/memory/status")
async def get_memory_status():
    stats = await db.get_context_stats()
    return {"status": "ok", "message_count": stats["count"], "total_chars": stats["total_chars"]}

@app.get("/api/chat_history")
async def get_chat_history(limit: int = 50):
    logs = await db.get_recent_context(limit=limit)
    # ç›´è¿‘é †ã§è¿”ã£ã¦ãã‚‹ã®ã§ã€æ™‚ç³»åˆ—é †ï¼ˆå¤ã„é †ï¼‰ã«ç›´ã—ã¦è¿”ã™
    return {"status": "ok", "logs": logs[::-1]}

@app.post("/api/memory/compact")
async def compact_memory():
    print("--- Starting Advanced Compaction ---")
    
    # 1. ä¼šè©±ãƒ­ã‚°ã‚’å…¨å–å¾—
    logs = await db.get_recent_context(limit=1000) # ååˆ†ãªé‡ã‚’å–å¾—
    if not logs:
        return {"status": "ok", "message": "No logs to compact.", "token_usage": 0}

    # ãƒ†ã‚­ã‚¹ãƒˆåŒ–
    conversation_text = ""
    for log in logs:
        conversation_text += f"{log['role']}: {log['content']}\n"

    # 2. å¸æ›¸AI (Librarian) ã«ã‚ˆã‚‹åˆ†æ
    librarian_prompt = """
    ã‚ãªãŸã¯ä¼šè©±ãƒ­ã‚°æ•´ç†ã®å°‚é–€AIã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±ãƒ­ã‚°ã‚’åˆ†æã—ã€é•·æœŸè¨˜æ†¶ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ã™ã¹ãé‡è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    
    ã€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    - å‡ºåŠ›ã¯ç®‡æ¡æ›¸ãã§ã™ãŒã€ä¸»èªï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ã‚„ã€Œãƒã‚¹ã‚¿ãƒ¼ã€ï¼‰ã¯æ—¢ã«åˆ†ã‹ã£ã¦ã„ã‚‹ã“ã¨ãªã®ã§ã€å¯èƒ½ãªé™ã‚Šçœç•¥ã—ã¦ãã ã•ã„ã€‚
    - ä¾‹ï¼šã€Œæ…å“‰ãƒã‚¹ã‚¿ãƒ¼ã¯ãƒªãƒ³ã‚´ãŒå¥½ãã€ â†’ ã€Œãƒªãƒ³ã‚´ãŒå¥½ãã€
    - å¤‰åŒ–ãŒã‚ã£ãŸç‚¹ã‚„ã€æ–°ã—ãåˆ¤æ˜ã—ãŸäº‹å®Ÿã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    
    å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§è¡Œã£ã¦ãã ã•ã„ï¼š
    {
      "user_updates": ["è¿½åŠ ã™ã¹ããƒ¦ãƒ¼ã‚¶ãƒ¼ã®äº‹æŸ„"],
      "identity_updates": ["è¿½åŠ ã™ã¹ãAIè‡ªèº«ã®äº‹æŸ„"],
      "memory_updates": ["è¿½åŠ ã™ã¹ãã‚¤ãƒ™ãƒ³ãƒˆã‚„çŸ¥è­˜"],
      "summary": "ä¼šè©±å…¨ä½“ã®ç°¡æ½”ãªè¦ç´„ï¼ˆ100æ–‡å­—ä»¥å†…ï¼‰"
    }
    """
    
    token_usage = {"prompt_token_count": 0, "candidates_token_count": 0, "total_token_count": 0}
    updates = {}
    
    if GEMINI_API_KEY:
        try:
            # åˆ†æç”¨ãƒ¢ãƒ‡ãƒ«
            librarian = genai.GenerativeModel(
                'gemini-3-flash-preview', 
                system_instruction=librarian_prompt,
                generation_config={"response_mime_type": "application/json"}
            )
            
            resp = await asyncio.to_thread(librarian.generate_content, conversation_text)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®å–å¾—ï¼ˆè©³ç´°ï¼‰
            if resp.usage_metadata:
                token_usage = {
                    "prompt_token_count": resp.usage_metadata.prompt_token_count,
                    "candidates_token_count": resp.usage_metadata.candidates_token_count,
                    "total_token_count": resp.usage_metadata.total_token_count
                }
            
            updates = json.loads(resp.text)
            print(f"Librarian Analysis: {updates}")
            
            # 3. ç·¨çº‚AI (Compiler) ã«ã‚ˆã‚‹æƒ…å ±ã®çµ±åˆã¨æ›´æ–°
            compiler_model = genai.GenerativeModel('gemini-3-flash-preview')

            async def update_file(filepath, new_info_list, category_name):
                if not new_info_list: return
                
                # æ—¢å­˜ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                current_content = ""
                if os.path.exists(filepath):
                    with open(filepath, "r", encoding="utf-8") as f:
                        current_content = f.read()
                
                # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                compiler_prompt = f"""
                ã‚ãªãŸã¯è¨˜æ†¶ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·¨çº‚è€…ã§ã™ã€‚
                ä»¥ä¸‹ã®ã€Œç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã€ã¨ã€Œæ–°ã—ãåˆ¤æ˜ã—ãŸæƒ…å ±ã€ã‚’å…ƒã«ã€æƒ…å ±ã‚’æ•´ç†ãƒ»çµ±åˆã—ã¦ã€æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
                
                ã€ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ ({category_name})ã€‘
                {current_content}
                
                ã€æ–°ã—ãåˆ¤æ˜ã—ãŸæƒ…å ±ã€‘
                {json.dumps(new_info_list, ensure_ascii=False)}
                
                ã€ç·¨é›†ãƒ«ãƒ¼ãƒ«ã€‘
                1. æƒ…å ±ãŒé‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã¯ã€ä¸€ã¤ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
                2. æ–°ã—ã„æƒ…å ±ãŒæ—¢å­˜ã®æƒ…å ±ã¨çŸ›ç›¾ã™ã‚‹å ´åˆã€æ–°ã—ã„æƒ…å ±ã‚’å„ªå…ˆã—ã¦æ›´æ–°ã—ã¦ãã ã•ã„ã€‚
                3. ä¼¼ãŸã‚ˆã†ãªæƒ…å ±ã¯ç®‡æ¡æ›¸ãã§ã¾ã¨ã‚ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚
                4. å‡ºåŠ›ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãã®ã‚‚ã®ï¼ˆMarkdownå½¢å¼ï¼‰ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
                5. ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ# User Profile ãªã©ï¼‰ã¯ç¶­æŒã—ã¦ãã ã•ã„ã€‚
                """
                
                try:
                    # ç·¨çº‚å®Ÿè¡Œ
                    resp = await asyncio.to_thread(compiler_model.generate_content, compiler_prompt)
                    new_content = resp.text.strip()
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ï¼ˆåŠ ç®—ï¼‰
                    if resp.usage_metadata:
                        token_usage["prompt_token_count"] += resp.usage_metadata.prompt_token_count
                        token_usage["candidates_token_count"] += resp.usage_metadata.candidates_token_count
                        token_usage["total_token_count"] += resp.usage_metadata.total_token_count

                    # å†…å®¹ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦æ›¸ãè¾¼ã¿ï¼ˆå®‰å…¨ç­–ï¼‰
                    if new_content and len(new_content) > 10:
                        with open(filepath, "w", encoding="utf-8") as f:
                            f.write(new_content)
                        print(f"â˜… Updated {category_name} Memory (ç·¨çº‚å®Œäº†)")
                    else:
                        print(f"âš  Warning: Empty response for {category_name}, skipping update.")
                        
                except Exception as e:
                    print(f"Compiler Error ({category_name}): {e}")

            # å„ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«æ›´æ–°ã‚’å®Ÿè¡Œ
            await update_file("memory/USER.md", updates.get("user_updates"), "User Profile")
            await update_file("memory/IDENTITY.md", updates.get("identity_updates"), "AI Identity")
            await update_file("memory/MEMORY.md", updates.get("memory_updates"), "Long Term Memory")

            # 4. ã‚³ãƒ³ãƒ‘ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã®ä¿å­˜
            summary_text = updates.get("summary", "No summary provided.")
            await db.log_compaction(
                summary=summary_text,
                start_id=0, 
                end_id=0,   
                token_usage=token_usage.get("total_token_count", 0),
                added_memories=updates
            )
            
            # 5. çŸ­æœŸè¨˜æ†¶ã®æ¶ˆå» (CompactionæˆåŠŸæ™‚ã®ã¿)
            await db.clear_logs()

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"Compaction Error: {error_detail}")
            return {"status": "error", "message": f"Compaction process failed: {str(e)}"}
    
    return {
        "status": "ok", 
        "message": "Smart Compaction complete.",
        "updates": updates,
        "token_usage": token_usage
    }

@app.get("/api/memory/compaction_logs")
async def get_compaction_logs(limit: int = 10):
    try:
        logs = await db.get_compaction_history(limit=limit)
        return {"status": "ok", "logs": logs}
    except Exception as e:
        return {"status": "error", "message": str(e)}

# æ—§ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã™ã‹ã€å‰Šé™¤ã—ã¦ã‚‚OK
@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    return {"status": "error", "text": "æ–°ã—ã„ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ /api/stream_chat ã‚’ä½¿ã£ã¦ã­ï¼"}

# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é…ä¿¡ã®è¨­å®š
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FRONTEND_DIR = os.path.join(os.path.dirname(BASE_DIR), "frontend")
app.mount("/", StaticFiles(directory=FRONTEND_DIR, html=True), name="static")
